{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# Import the Iris dataset using Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert the dataset from a pandas DataFrame to a PyTorch tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Transform the species labels to numerical values\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "\n",
    "# Assign numerical labels based on species\n",
    "# labels[iris.species == 'setosa'] = 0  # This line isn't needed as 'setosa' already corresponds to 0\n",
    "labels[iris.species == 'versicolor'] = 1  # Assign label 1 to 'versicolor'\n",
    "labels[iris.species == 'virginica'] = 2   # Assign label 2 to 'virginica'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-GvjKA8AesK"
   },
   "source": [
    "# Separate the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVrlOHYIAg0r"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using scikit-learn\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Convert the data and labels into PyTorch Datasets\n",
    "train_data = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoader objects for efficient batch processing\n",
    "batch_size = 16\n",
    "\n",
    "# Training DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# - DataLoader is a PyTorch utility for handling batches of data.\n",
    "# - train_data contains the training samples and their corresponding labels.\n",
    "# - batch_size determines the number of samples in each batch during training.\n",
    "# - shuffle=True means that the data will be randomly shuffled before each epoch, which helps prevent the model from overfitting to the order of the data.\n",
    "\n",
    "# Testing DataLoader\n",
    "test_loader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])\n",
    "# - For testing, we typically load the entire testing dataset at once, which is equivalent to using a batch size equal to the number of testing samples.\n",
    "# - test_data contains the testing samples and labels.\n",
    "# - batch_size=test_data.tensors[0].shape[0] specifies that the batch size for testing should be equal to the number of testing samples, effectively loading the entire testing dataset in one batch.\n",
    "# - The batch size for testing can be set this way since there's no need for batching during testing, and it simplifies the code.\n",
    "\n",
    "# Note: You might wonder why we use DataLoader for testing with a batch size equal to the number of samples. This is a common practice in PyTorch, and it allows us to use the same testing loop structure as the training loop without modification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPTMij-sQo9i"
   },
   "source": [
    "# Create the model and a training regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0qe1q9nRwPG"
   },
   "outputs": [],
   "source": [
    "# Define a custom neural network model class called theModelClass\n",
    "class theModelClass(nn.Module):\n",
    "    def __init__(self, dropoutRate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "        self.input = nn.Linear(4, 12)  # Input layer with 4 input features and 12 output units\n",
    "        self.hidden = nn.Linear(12, 12)  # Hidden layer with 12 input units and 12 output units\n",
    "        self.output = nn.Linear(12, 3)  # Output layer with 12 input units and 3 output units (for classification)\n",
    "\n",
    "        # Store the specified dropout rate as a model parameter\n",
    "        self.dr = dropoutRate\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Input Layer: Pass the data through the input layer and apply ReLU activation\n",
    "        x = F.relu(self.input(x))\n",
    "\n",
    "        # Apply dropout after the input layer\n",
    "        x = F.dropout(x, p=self.dr, training=self.training)\n",
    "        # - During training, dropout is applied to regularize the model.\n",
    "        # - During evaluation (eval mode), dropout is turned off, ensuring consistent inference.\n",
    "\n",
    "        # Hidden Layer: Pass the data through the hidden layer and apply ReLU activation\n",
    "        x = F.relu(self.hidden(x))\n",
    "\n",
    "        # Apply dropout after the hidden layer\n",
    "        x = F.dropout(x, p=self.dr, training=self.training)\n",
    "\n",
    "        # Output Layer: Pass the data through the output layer (no dropout applied here)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZnfbIcOTfzm"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the custom neural network model class called tmpnet\n",
    "# with a dropout rate of 0.25.\n",
    "tmpnet = theModelClass(0.25)\n",
    "\n",
    "# Generate random data (tmpdata) with shape (10, 4), where 10 is the number of data points\n",
    "# and 4 is the number of input features. This simulates input data for testing.\n",
    "tmpdata = torch.randn((10, 4))\n",
    "\n",
    "# Pass the random data (tmpdata) through the tmpnet model to obtain predictions (yhat).\n",
    "yhat = tmpnet(tmpdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0JMIGb1iV_9"
   },
   "outputs": [],
   "source": [
    "# Function to create a new ANN model\n",
    "def createANewModel(dropoutrate):\n",
    "\n",
    "  # Create an instance of the custom neural network model class 'theModelClass'\n",
    "  ANNiris = theModelClass(dropoutrate)\n",
    "\n",
    "  # Define the loss function as Cross-Entropy Loss, which is suitable for classification tasks\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Create an optimizer for training the model. Here, we use Stochastic Gradient Descent (SGD).\n",
    "  # We pass the model's parameters to the optimizer so that it knows which weights to update during training.\n",
    "  # We set the learning rate to 0.005, which controls the step size in weight updates.\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(), lr=0.005)\n",
    "\n",
    "  # Return the created model, loss function, and optimizer.\n",
    "  return ANNiris, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "outputs": [],
   "source": [
    "# Global parameter: Number of epochs for training\n",
    "numepochs = 500\n",
    "\n",
    "# Function to train the model\n",
    "def trainTheModel():\n",
    "\n",
    "  # Initialize lists to store training and test accuracies\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "\n",
    "  # Loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # Switch the model to training mode\n",
    "    ANNiris.train()\n",
    "\n",
    "    # Initialize a list to store batch accuracies\n",
    "    batchAcc = []\n",
    "\n",
    "    # Loop over training data batches\n",
    "    for X, y in train_loader:\n",
    "\n",
    "      # Forward pass and compute the loss\n",
    "      yHat = ANNiris(X)  # Forward pass to obtain predictions\n",
    "      loss = lossfun(yHat, y)  # Compute the loss using the specified loss function\n",
    "\n",
    "      # Backpropagation\n",
    "      optimizer.zero_grad()  # Reset gradients\n",
    "      loss.backward()  # Compute gradients\n",
    "      optimizer.step()  # Update model parameters\n",
    "\n",
    "      # Compute training accuracy just for this batch\n",
    "      batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()).item())\n",
    "\n",
    "    # End of batch loop...\n",
    "\n",
    "    # Calculate the average training accuracy for this epoch\n",
    "    trainAcc.append(np.mean(batchAcc))\n",
    "\n",
    "    # Test accuracy\n",
    "    ANNiris.eval()  # Switch the model to evaluation mode (dropout turned off)\n",
    "    X, y = next(iter(test_loader))  # Extract a batch of test data (X, y)\n",
    "    predlabels = torch.argmax(ANNiris(X), axis=1)  # Predict labels for the test data\n",
    "    testAcc.append(100 * torch.mean((predlabels == y).float()).item())\n",
    "\n",
    "  # Function output: Lists of training and test accuracies over epochs\n",
    "  return trainAcc, testAcc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKteRdsnQx15"
   },
   "source": [
    "# Now for the real work! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXku7xIdcu7Y"
   },
   "outputs": [],
   "source": [
    "# Create a model with dropout rate set to 0.0\n",
    "dropoutrate = .0\n",
    "ANNiris, lossfun, optimizer = createANewModel(dropoutrate)\n",
    "\n",
    "# Train the model\n",
    "trainAcc, testAcc = trainTheModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(trainAcc, 'bs-')\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.plot(testAcc, 'ro-')\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# Create a legend for the plot\n",
    "plt.legend(['Train', 'Test'])\n",
    "\n",
    "# Set the title of the plot, including the dropout rate value\n",
    "plt.title('Dropout rate = %g' % dropoutrate)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLGPKiXKQCpE"
   },
   "outputs": [],
   "source": [
    "# Run an experiment\n",
    "\n",
    "# Define a range of dropout rates from 0 to 0.9 in increments of 0.1\n",
    "dropoutRates = np.arange(10)/10\n",
    "\n",
    "# Create an array to store the results with two columns: training accuracy and test accuracy\n",
    "results = np.zeros((len(dropoutRates), 2))\n",
    "\n",
    "# Loop over different dropout rates\n",
    "for di in range(len(dropoutRates)):\n",
    "\n",
    "    # Create and train the model for the current dropout rate\n",
    "    ANNiris, lossfun, optimizer = createANewModel(dropoutRates[di])\n",
    "    trainAcc, testAcc = trainTheModel()\n",
    "\n",
    "    # Store the last 50 epochs' average accuracies in the results array\n",
    "    results[di, 0] = np.mean(trainAcc[-50:])\n",
    "    results[di, 1] = np.mean(testAcc[-50:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPLChql1ZTf7"
   },
   "outputs": [],
   "source": [
    "# Plot the experiment results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot average accuracy vs. dropout proportion\n",
    "ax[0].plot(dropoutRates, results, 'o-')\n",
    "ax[0].set_xlabel('Dropout proportion')\n",
    "ax[0].set_ylabel('Average accuracy')\n",
    "ax[0].legend(['Train', 'Test'])\n",
    "\n",
    "# Plot the difference between training and test accuracy vs. dropout proportion\n",
    "ax[1].plot(dropoutRates, -np.diff(results, axis=1), 'o-')\n",
    "ax[1].plot([0, .9], [0, 0], 'k--')\n",
    "ax[1].set_xlabel('Dropout proportion')\n",
    "ax[1].set_ylabel('Train-test difference (acc%)')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2SWDR15s0Z6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPQBjBoXs0rY"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL0w01fAs1B4"
   },
   "outputs": [],
   "source": [
    "# 1) Remove the ReLU nonlinearity from the network. Does that change the effect of dropout proportion on performance?\n",
    "# \n",
    "# 2) I mentioned that dropout doesn't necessarily improve performance for shallow models. What happens if you increase \n",
    "#    the complexity of this model, for example by adding several additional (and wider) hidden layers?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvAeh+aul0pKj0jD00L8Ht",
   "collapsed_sections": [],
   "name": "DUDL_regular_dropout_example2.ipynb",
   "provenance": [
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1616941708388
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
