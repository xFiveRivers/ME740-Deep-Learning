{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5082,
     "status": "ok",
     "timestamp": 1673017817069,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataloader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1673017818322,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# Import the Iris dataset using seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset into a pandas DataFrame\n",
    "iris = sns.load_dataset('iris')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1673017818323,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "vJPkH6Bfh01_"
   },
   "outputs": [],
   "source": [
    "# Organize the data\n",
    "\n",
    "# Convert data from a pandas DataFrame to a PyTorch tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Transform species labels to numerical values\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "\n",
    "# Set numerical values for species labels\n",
    "labels[iris.species == 'versicolor'] = 1\n",
    "labels[iris.species == 'virginica'] = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-GvjKA8AesK"
   },
   "source": [
    "# Break the data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1673017818323,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "OVrlOHYIAg0r"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "# Use scikit-learn to split the data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# Convert the split data into PyTorch Datasets (already converted to tensors)\n",
    "\n",
    "# Create a PyTorch TensorDataset for training data and labels\n",
    "train_dataDataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "\n",
    "# Create a PyTorch TensorDataset for test data and labels\n",
    "test_dataDataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoader objects for efficient data handling during training and testing\n",
    "\n",
    "# Create a DataLoader for training data with a batch size of 64, shuffling the data, and dropping the last batch if it's incomplete\n",
    "train_loader = DataLoader(train_dataDataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# Create a DataLoader for test data with a batch size equal to the size of the test dataset\n",
    "test_loader = DataLoader(test_dataDataset, batch_size=test_dataDataset.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlJfWqK6RlK3"
   },
   "source": [
    "# the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1673017851056,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "v0JMIGb1iV_9"
   },
   "outputs": [],
   "source": [
    "# A function that creates an Artificial Neural Network (ANN) model\n",
    "\n",
    "def createANewModel(L2lambda):\n",
    "\n",
    "  # Define the architecture of the ANN model\n",
    "\n",
    "  # Create an instance of nn.Sequential to define a sequential neural network model\n",
    "  ANNiris = nn.Sequential(\n",
    "      nn.Linear(4, 64),    # Input layer with 4 input features and 64 output units\n",
    "      nn.ReLU(),           # ReLU (Rectified Linear Unit) activation function\n",
    "      nn.Linear(64, 64),   # Hidden layer with 64 input units and 64 output units\n",
    "      nn.ReLU(),           # ReLU activation function\n",
    "      nn.Linear(64, 3)     # Output layer with 64 input units and 3 output units (for classification)\n",
    "  )\n",
    "\n",
    "  # Define the loss function\n",
    "\n",
    "  # Use CrossEntropyLoss for multi-class classification problems\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Define the optimizer\n",
    "\n",
    "  # Use Stochastic Gradient Descent (SGD) optimizer to update model parameters\n",
    "  # Set the learning rate to 0.005 and apply L2 regularization with the specified lambda (L2lambda)\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(), lr=0.005, weight_decay=L2lambda)\n",
    "\n",
    "  # Return the created ANN model, loss function, and optimizer\n",
    "  return ANNiris, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1673017851643,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "cVD1nFTli7TO"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Global parameter: number of epochs\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel():\n",
    "\n",
    "  # Initialize lists to store training accuracies, test accuracies, and losses\n",
    "  trainAcc = []\n",
    "  testAcc = []\n",
    "  losses = []\n",
    "\n",
    "  # Loop over training epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # Loop over training data batches\n",
    "    batchAcc = []  # List to store batch accuracies\n",
    "    batchLoss = []  # List to store batch losses\n",
    "    for X, y in train_loader:\n",
    "\n",
    "      # Forward pass and compute the loss\n",
    "      yHat = ANNiris(X)  # Obtain model predictions\n",
    "      loss = lossfun(yHat, y)  # Compute the loss using the specified loss function\n",
    "      \n",
    "      # Backpropagation: zero gradients, compute gradients, and update model weights\n",
    "      optimizer.zero_grad()  # Zero out the gradients\n",
    "      loss.backward()  # Compute gradients using backpropagation\n",
    "      optimizer.step()  # Update model weights based on gradients\n",
    "\n",
    "      # Compute training accuracy for this batch\n",
    "      batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()).item())\n",
    "      batchLoss.append(loss.item())  # Append the batch loss to the list\n",
    "    # End of batch loop...\n",
    "\n",
    "    # Calculate and store the average training accuracy and loss for this epoch\n",
    "    trainAcc.append(np.mean(batchAcc))\n",
    "    losses.append(np.mean(batchLoss))\n",
    "\n",
    "    # Evaluate the model on the test data to compute test accuracy\n",
    "    ANNiris.eval()  # Set the model to evaluation mode\n",
    "    X, y = next(iter(test_loader))  # Extract a batch of test data\n",
    "    predlabels = torch.argmax(ANNiris(X), axis=1)  # Get predicted labels\n",
    "    testAcc.append(100 * torch.mean((predlabels == y).float()).item())  # Compute and store test accuracy\n",
    "\n",
    "    # Reset the model to training mode\n",
    "    ANNiris.train()\n",
    "\n",
    "  # Function output: training accuracies, test accuracies, and losses\n",
    "  return trainAcc, testAcc, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2128,
     "status": "ok",
     "timestamp": 1673017854173,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -420
    },
    "id": "vXku7xIdcu7Y"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "# Regularization parameter (L2 lambda)\n",
    "L2lambda = .01\n",
    "\n",
    "# Create a new neural network model using the function createANewModel\n",
    "# This function returns the model (ANNiris), loss function (lossfun), and optimizer\n",
    "ANNiris, lossfun, optimizer = createANewModel(L2lambda)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Initialize lists to store training accuracies, test accuracies, and losses\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "losses = []\n",
    "\n",
    "# Loop over training epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    # Loop over training data batches\n",
    "    batchAcc = []  # List to store batch accuracies\n",
    "    batchLoss = []  # List to store batch losses\n",
    "    for X, y in train_loader:\n",
    "\n",
    "        # Forward pass and compute the loss\n",
    "        yHat = ANNiris(X)  # Obtain model predictions\n",
    "        loss = lossfun(yHat, y)  # Compute the loss using the specified loss function\n",
    "      \n",
    "        # Backpropagation: zero gradients, compute gradients, and update model weights\n",
    "        optimizer.zero_grad()  # Zero out the gradients\n",
    "        loss.backward()  # Compute gradients using backpropagation\n",
    "        optimizer.step()  # Update model weights based on gradients\n",
    "\n",
    "        # Compute training accuracy for this batch\n",
    "        batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()).item())\n",
    "        batchLoss.append(loss.item())  # Append the batch loss to the list\n",
    "\n",
    "    # End of batch loop...\n",
    "\n",
    "    # Calculate and store the average training accuracy and loss for this epoch\n",
    "    trainAcc.append(np.mean(batchAcc))\n",
    "    losses.append(np.mean(batchLoss))\n",
    "\n",
    "    # Evaluate the model on the test data to compute test accuracy\n",
    "    ANNiris.eval()  # Set the model to evaluation mode\n",
    "    X, y = next(iter(test_loader))  # Extract a batch of test data\n",
    "    predlabels = torch.argmax(ANNiris(X), axis=1)  # Get predicted labels\n",
    "    testAcc.append(100 * torch.mean((predlabels == y).float()).item())  # Compute and store test accuracy\n",
    "\n",
    "    # Reset the model to training mode\n",
    "    ANNiris.train()\n",
    "\n",
    "# Function output: training accuracies, test accuracies, and losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot the training losses on the first subplot (left)\n",
    "ax[0].plot(losses, 'k^-')  # Plot losses as black triangles\n",
    "ax[0].set_ylabel('Loss')  # Set the y-axis label to 'Loss'\n",
    "ax[0].set_xlabel('Epochs')  # Set the x-axis label to 'Epochs'\n",
    "ax[0].set_title('Losses with L2 $\\lambda$=' + str(L2lambda))  # Set the title\n",
    "\n",
    "# Plot the training and test accuracies on the second subplot (right)\n",
    "ax[1].plot(trainAcc, 'ro-')  # Plot training accuracies as red circles\n",
    "ax[1].plot(testAcc, 'bs-')  # Plot test accuracies as blue squares\n",
    "ax[1].set_title('Accuracy with L2 $\\lambda$=' + str(L2lambda))  # Set the title\n",
    "ax[1].set_xlabel('Epochs')  # Set the x-axis label to 'Epochs'\n",
    "ax[1].set_ylabel('Accuracy (%)')  # Set the y-axis label to 'Accuracy (%)'\n",
    "ax[1].legend(['Train', 'Test'])  # Add a legend with labels 'Train' and 'Test'\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BQ99x9GJbJT"
   },
   "outputs": [],
   "source": [
    "# Create a 1D smoothing filter\n",
    "def smooth(x, k):\n",
    "    # Use np.convolve to apply a smoothing filter to the input data 'x'\n",
    "    # np.ones(k) creates a filter with all elements set to 1, representing the weights of the filter.\n",
    "    # / k divides each element of the filter by 'k', which averages the values in the filter window.\n",
    "    # 'mode='same'' ensures the output has the same length as the input data 'x'.\n",
    "    return np.convolve(x, np.ones(k) / k, mode='same')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK9DwkLwRpl2"
   },
   "source": [
    "# The experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1iP8hhNdwV7"
   },
   "outputs": [],
   "source": [
    "# Define a range of L2 regularization amounts\n",
    "l2lambdas = np.linspace(0, 0.1, 10)\n",
    "\n",
    "# Initialize output results matrices for training and testing accuracies\n",
    "accuracyResultsTrain = np.zeros((numepochs, len(l2lambdas)))\n",
    "accuracyResultsTest  = np.zeros((numepochs, len(l2lambdas)))\n",
    "\n",
    "# Loop over different L2 regularization values\n",
    "for li in range(len(l2lambdas)):\n",
    "\n",
    "    # Create and train a model with the current L2 regularization value\n",
    "    ANNiris, lossfun, optimizer = createANewModel(l2lambdas[li])\n",
    "    trainAcc, testAcc, losses = trainTheModel()\n",
    "\n",
    "    # Store the smoothed training and testing accuracies in the respective result matrices\n",
    "    accuracyResultsTrain[:, li] = smooth(trainAcc, 10)\n",
    "    accuracyResultsTest[:, li]  = smooth(testAcc, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWPlIN9Gel79"
   },
   "outputs": [],
   "source": [
    "# Create subplots for displaying training and testing accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(17, 7))\n",
    "\n",
    "# Plot training accuracy\n",
    "ax[0].plot(accuracyResultsTrain)\n",
    "ax[0].set_title('Train accuracy')\n",
    "\n",
    "# Plot testing accuracy\n",
    "ax[1].plot(accuracyResultsTest)\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Create labels for the legend based on L2 regularization strengths\n",
    "leglabels = [np.round(i, 2) for i in l2lambdas]\n",
    "\n",
    "# Common features for both subplots\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)          # Add legend with L2 regularization strengths\n",
    "    ax[i].set_xlabel('Epoch')        # Label the x-axis as \"Epoch\"\n",
    "    ax[i].set_ylabel('Accuracy (%)') # Label the y-axis as \"Accuracy (%)\"\n",
    "    ax[i].set_ylim([50, 101])       # Set y-axis limit to range from 50 to 100 (percentage scale)\n",
    "    ax[i].grid()                     # Add gridlines to the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeVhsO__fDKH"
   },
   "outputs": [],
   "source": [
    "# Specify the range of epochs for which to calculate average accuracy\n",
    "epoch_range = [500, 950]\n",
    "\n",
    "# Plot the average accuracy by L2 regularization rate for training data\n",
    "plt.plot(\n",
    "    l2lambdas,\n",
    "    np.mean(accuracyResultsTrain[epoch_range[0]:epoch_range[1], :], axis=0),\n",
    "    'bo-',  # Blue circles with solid lines\n",
    "    label='TRAIN'  # Label for the training data\n",
    ")\n",
    "\n",
    "# Plot the average accuracy by L2 regularization rate for testing data\n",
    "plt.plot(\n",
    "    l2lambdas,\n",
    "    np.mean(accuracyResultsTest[epoch_range[0]:epoch_range[1], :], axis=0),\n",
    "    'rs-',  # Red squares with solid lines\n",
    "    label='TEST'  # Label for the testing data\n",
    ")\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('L2 regularization amount')\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Add a legend to distinguish between training and testing data\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRy0bwSp4F5W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OK4WCZnQ1Up"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ElMMMxdQ3cq"
   },
   "outputs": [],
   "source": [
    "# 1) In general, regularization tends to benefit large, complex models, and has less impact (and sometimes even a negative\n",
    "#    impact) on smaller or simpler model architectures. Modify the model architecture to have three hidden layers, and\n",
    "#    see whether that changes the effect of L2 regularization on performance. (You might want to increase the number of \n",
    "#    epochs.)\n",
    "# \n",
    "# 2) Multiple regularization methods can be combined. Add 15% dropout to the hidden layer(s) and see how that affects\n",
    "#    the model's performance. \n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNeQ1lY2eiOH+Eiyh96C732",
   "provenance": [
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
