{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# \n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# import dataset (comes with seaborn)\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJPkH6Bfh01_"
   },
   "outputs": [],
   "source": [
    "# organize the data\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "# labels[iris.species=='setosa']   = 0 # don't need!\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-GvjKA8AesK"
   },
   "source": [
    "# Break the data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVrlOHYIAg0r"
   },
   "outputs": [],
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.2)\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_dataDataset = torch.utils.data.TensorDataset(train_data,train_labels)\n",
    "test_dataDataset  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
    "\n",
    "# create dataloader objects\n",
    "train_loader = DataLoader(train_dataDataset,batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataDataset,batch_size=test_dataDataset.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z23MkPyYRMCc"
   },
   "source": [
    "# Specify the model architecture and training plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0JMIGb1iV_9"
   },
   "outputs": [],
   "source": [
    "# A function that creates the ANN model\n",
    "def createANewModel():\n",
    "\n",
    "  # Define the model architecture using nn.Sequential\n",
    "  ANNiris = nn.Sequential(\n",
    "      nn.Linear(4, 64),   # Input layer: 4 input features, 64 output units\n",
    "      nn.ReLU(),           # Activation function: ReLU (Rectified Linear Unit)\n",
    "      nn.Linear(64, 64),  # Hidden layer: 64 input units from the previous layer, 64 output units\n",
    "      nn.ReLU(),           # Activation function: ReLU\n",
    "      nn.Linear(64, 3),   # Output layer: 64 input units from the previous layer, 3 output units\n",
    "  )\n",
    "\n",
    "  # Define the loss function as Cross-Entropy Loss\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Define the optimizer as Stochastic Gradient Descent (SGD)\n",
    "  # It optimizes the model's parameters with a learning rate of 0.005\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(), lr=0.005)\n",
    "\n",
    "  # Return the model, loss function, and optimizer\n",
    "  return ANNiris, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gaPXT_kO-GM"
   },
   "outputs": [],
   "source": [
    "# Create a temporary model for exploration\n",
    "tmpmodel = createANewModel()[0]\n",
    "\n",
    "# Print the model architecture\n",
    "print(tmpmodel)\n",
    "\n",
    "# Print the model's parameters\n",
    "for i in tmpmodel.named_parameters():\n",
    "    print(i[0], i[1].shape, i[1].numel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DrBMhYbRTnY"
   },
   "source": [
    "# Function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "outputs": [],
   "source": [
    "# Train the model with L1 regularization\n",
    "\n",
    "# Global parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel(L1lambda):\n",
    "\n",
    "    # Initialize lists for tracking training metrics\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "    losses = []\n",
    "\n",
    "    # Count the total number of weights in the model\n",
    "    nweights = 0\n",
    "    for pname, weight in ANNiris.named_parameters():\n",
    "        if 'bias' not in pname:\n",
    "            nweights += weight.numel()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # Lists to store batch accuracy and loss\n",
    "        batchAcc = []\n",
    "        batchLoss = []\n",
    "\n",
    "        # Loop over training data batches\n",
    "        for X, y in train_loader:\n",
    "\n",
    "            # Forward pass and loss\n",
    "            yHat = ANNiris(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "            # Add L1 regularization term\n",
    "            L1_term = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "            # Sum up all abs(weights)\n",
    "            for pname, weight in ANNiris.named_parameters():\n",
    "                if 'bias' not in pname:\n",
    "                    L1_term += torch.sum(torch.abs(weight))\n",
    "\n",
    "            # Add L1 term to the loss\n",
    "            loss = loss + L1lambda * L1_term / nweights\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute training accuracy just for this batch\n",
    "            batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()).item())\n",
    "            batchLoss.append(loss.item())\n",
    "        # End of batch loop...\n",
    "\n",
    "        # Calculate the average training accuracy and loss for this epoch\n",
    "        trainAcc.append(np.mean(batchAcc))\n",
    "        losses.append(np.mean(batchLoss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X, y = next(iter(test_loader))  # Extract X,y from test dataloader\n",
    "        predlabels = torch.argmax(ANNiris(X), axis=1)\n",
    "        testAcc.append(100 * torch.mean((predlabels == y).float()).item())\n",
    "\n",
    "    # Function output\n",
    "    return trainAcc, testAcc, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2q3a4-wRX3X"
   },
   "source": [
    "# Bring it to life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXku7xIdcu7Y"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "# Create an instance of the ANN model, specify the loss function, and optimizer\n",
    "ANNiris, lossfun, optimizer = createANewModel()\n",
    "\n",
    "# Set the L1 regularization parameter\n",
    "L1lambda = 0.001\n",
    "\n",
    "# Train the model with L1 regularization\n",
    "trainAcc, testAcc, losses = trainTheModel(L1lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot the losses\n",
    "ax[0].plot(losses, 'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with L1 $\\lambda$=' + str(L1lambda))\n",
    "\n",
    "# Plot the training and test accuracies\n",
    "ax[1].plot(trainAcc, 'ro-')\n",
    "ax[1].plot(testAcc, 'bs-')\n",
    "ax[1].set_title('Accuracy with L1 $\\lambda$=' + str(L1lambda))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train', 'Test'])\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BQ99x9GJbJT"
   },
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter\n",
    "\n",
    "# Define a Python function called smooth that performs 1D smoothing on a given array of values.\n",
    "# The function takes two arguments:\n",
    "# - x: The input array to be smoothed.\n",
    "# - k: An integer specifying the smoothing window size.\n",
    "\n",
    "def smooth(x, k):\n",
    "    # Use NumPy's convolve function to perform the smoothing.\n",
    "    # np.ones(k) creates a 1D kernel of size k, where all elements are set to 1.\n",
    "    # np.ones(k) / k normalizes the kernel by dividing each element by k.\n",
    "    # This creates a kernel that will compute the weighted moving average of the input data.\n",
    "    # The mode='same' argument ensures that the output has the same shape as the input.\n",
    "    return np.convolve(x, np.ones(k) / k, mode='same')\n",
    "\n",
    "# In summary, this smooth function takes an input array x and applies a weighted moving average\n",
    "# smoothing operation with a specified window size k. The result is a smoothed array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf17iO7NRdEn"
   },
   "source": [
    "# Now for the parameteric experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1iP8hhNdwV7"
   },
   "outputs": [],
   "source": [
    "# Define a range of L1 regularization amounts (L1lambda) using NumPy's linspace function.\n",
    "# This creates an array of 10 evenly spaced values ranging from 0 to 0.005.\n",
    "L1lambda = np.linspace(0, 0.005, 10)\n",
    "\n",
    "# Initialize empty matrices to store training and testing accuracy results.\n",
    "# These matrices will have dimensions (numepochs, len(L1lambda)).\n",
    "# numepochs is a global parameter representing the number of training epochs.\n",
    "accuracyResultsTrain = np.zeros((numepochs, len(L1lambda)))\n",
    "accuracyResultsTest = np.zeros((numepochs, len(L1lambda)))\n",
    "\n",
    "# Loop over different L1 regularization values.\n",
    "for li in range(len(L1lambda)):\n",
    "\n",
    "    # Create a new ANN model, specify the loss function, and optimizer.\n",
    "    ANNiris, lossfun, optimizer = createANewModel()\n",
    "\n",
    "    # Train the model with the current L1 regularization value (L1lambda[li]).\n",
    "    trainAcc, testAcc, losses = trainTheModel(L1lambda[li])\n",
    "\n",
    "    # Smooth the training and testing accuracy curves using a smoothing window of 10.\n",
    "    # This helps reduce noise in the accuracy curves, making them easier to interpret.\n",
    "    accuracyResultsTrain[:, li] = smooth(trainAcc, 10)\n",
    "    accuracyResultsTest[:, li] = smooth(testAcc, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgY8DU_WTM6T"
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots (1 row, 2 columns) and set the overall size to (17,7).\n",
    "fig, ax = plt.subplots(1, 2, figsize=(17, 7))\n",
    "\n",
    "# Plot the training accuracy results in the first subplot (ax[0]).\n",
    "ax[0].plot(accuracyResultsTrain)\n",
    "ax[0].set_title('Train accuracy')  # Set the title of the subplot to 'Train accuracy'.\n",
    "\n",
    "# Plot the testing accuracy results in the second subplot (ax[1]).\n",
    "ax[1].plot(accuracyResultsTest)\n",
    "ax[1].set_title('Test accuracy')  # Set the title of the subplot to 'Test accuracy'.\n",
    "\n",
    "# Create a list of labels for the legend based on the L1 regularization values (L1lambda).\n",
    "# These labels are rounded to four decimal places for readability.\n",
    "leglabels = [np.round(i, 4) for i in L1lambda]\n",
    "\n",
    "# Loop over the two subplots (ax[0] and ax[1]) for common formatting.\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)  # Add the legend with the L1 regularization values.\n",
    "    ax[i].set_xlabel('Epoch')  # Set the x-axis label to 'Epoch'.\n",
    "    ax[i].set_ylabel('Accuracy (%)')  # Set the y-axis label to 'Accuracy (%)'.\n",
    "    ax[i].set_ylim([50, 101])  # Set the y-axis limits for accuracy values between 50% and 101%.\n",
    "    ax[i].grid()  # Add gridlines to the subplot.\n",
    "\n",
    "# Display the plot with both training and testing accuracy curves.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWPlIN9Gel79"
   },
   "outputs": [],
   "source": [
    "# Define an epoch range for which you want to calculate the average accuracy.\n",
    "epoch_range = [500, 950]\n",
    "\n",
    "# Create a line plot to show the average accuracy by L1 regularization amount.\n",
    "# Plot the L1 regularization values (L1lambda) on the x-axis.\n",
    "# Calculate the mean of accuracyResultsTrain and accuracyResultsTest within the specified epoch range.\n",
    "# Plot the average training accuracy with blue circles ('bo-') and label it as 'TRAIN'.\n",
    "# Plot the average testing accuracy with red squares ('rs-') and label it as 'TEST'.\n",
    "# Set the x-axis label to 'L1 regularization amount'.\n",
    "# Set the y-axis label to 'Accuracy'.\n",
    "# Add a legend to distinguish between training and testing accuracy.\n",
    "plt.plot(L1lambda,\n",
    "         np.mean(accuracyResultsTrain[epoch_range[0]:epoch_range[1], :], axis=0),\n",
    "         'bo-', label='TRAIN')\n",
    "\n",
    "plt.plot(L1lambda,\n",
    "         np.mean(accuracyResultsTest[epoch_range[0]:epoch_range[1], :], axis=0),\n",
    "         'rs-', label='TEST')\n",
    "\n",
    "# Set the x-axis label to 'L1 regularization amount'.\n",
    "plt.xlabel('L1 regularization amount')\n",
    "\n",
    "# Set the y-axis label to 'Accuracy'.\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Add a legend to the plot to distinguish between training and testing accuracy.\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot with average accuracy values for different L1 regularization amounts.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSKpl7coA_-U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTe7Ep0NYX6F"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qs2EazKSVHEe"
   },
   "outputs": [],
   "source": [
    "# 1) In the previous video we used a pytorch function to implement L2 regularization, and in this video we implemented \n",
    "#    L1 regularization manually. Modify the code here to create a manual L2 regularizer.\n",
    "# \n",
    "# 2) Based on your modification above, create a combined L1+L2 regularizer. Does it make sense to use the same lambda\n",
    "#    parameter, or do you think it should be adjusted?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/jCS6UOSEOVtqEnGU0c0p",
   "collapsed_sections": [],
   "name": "DUDL_regular_L1regu.ipynb",
   "provenance": [
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617255660148
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
