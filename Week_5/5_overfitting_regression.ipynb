{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz6w7TtgQ6QF"
   },
   "source": [
    "# Create the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-SP8NPsMNRL"
   },
   "outputs": [],
   "source": [
    "# Generating Random Data:\n",
    "# We are creating synthetic data for a simple scatter plot.\n",
    "# N represents the number of data points we want to generate.\n",
    "N = 100\n",
    "\n",
    "# Generating Random x Values:\n",
    "# We use PyTorch's `torch.randn()` function to generate N random values.\n",
    "# These values represent the x-coordinates of our data points.\n",
    "x = torch.randn(N, 1)\n",
    "\n",
    "# Generating Random y Values:\n",
    "# We add another set of random values to x to create a simple linear relationship.\n",
    "# This simulates a scenario where y is somewhat dependent on x but with some random noise.\n",
    "y = x + torch.randn(N, 1)\n",
    "\n",
    "# Creating a Scatter Plot:\n",
    "# We use Matplotlib, a popular Python plotting library, to create a scatter plot.\n",
    "# The 's' argument specifies that we want to use square markers for the data points.\n",
    "# We are plotting x on the x-axis and y on the y-axis.\n",
    "plt.plot(x, y, 's')\n",
    "\n",
    "# Displaying the Plot:\n",
    "# Finally, we use plt.show() to display the scatter plot.\n",
    "# This visualizes our generated data points on a 2D plane.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krQeh5wYMNla"
   },
   "outputs": [],
   "source": [
    "# Building a Simple Neural Network Model:\n",
    "# We are defining a simple neural network model using PyTorch's nn.Sequential.\n",
    "# This model consists of an input layer, an activation function, and an output layer.\n",
    "\n",
    "# Initialize the Model:\n",
    "# We create an instance of nn.Sequential to define our neural network model.\n",
    "ANNreg = nn.Sequential(\n",
    "\n",
    "    # Input Layer:\n",
    "    # The input layer has 1 input feature (1-dimensional input).\n",
    "    nn.Linear(1, 1),  # Input layer with 1 input neuron and 1 output neuron.\n",
    "\n",
    "    # Activation Function:\n",
    "    # We apply the Rectified Linear Unit (ReLU) activation function.\n",
    "    # ReLU is commonly used in hidden layers to introduce non-linearity.\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Output Layer:\n",
    "    # The output layer has 1 neuron, which is the final output of our model.\n",
    "    nn.Linear(1, 1)   # Output layer with 1 input neuron and 1 output neuron.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmHh7GrvMNoy"
   },
   "outputs": [],
   "source": [
    "# Setting Model Meta-Parameters:\n",
    "# In this code cell, we specify various meta-parameters for training our neural network model.\n",
    "\n",
    "# Learning Rate:\n",
    "# Learning rate determines the step size during optimization.\n",
    "# It controls how quickly the model adjusts its parameters based on the loss.\n",
    "# A smaller learning rate may lead to slower convergence but better stability.\n",
    "learningRate = 0.05\n",
    "\n",
    "# Loss Function:\n",
    "# We choose Mean Squared Error (MSE) Loss for this regression task.\n",
    "# MSE is commonly used for regression problems, where the goal is to minimize the\n",
    "# squared difference between the predicted and actual values.\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# Optimizer:\n",
    "# Stochastic Gradient Descent (SGD) is a popular optimization algorithm.\n",
    "# We use SGD to update the model's parameters during training.\n",
    "# The learning rate (lr) determines the step size in each update.\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(), lr=learningRate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpZsJzRKQ-xM"
   },
   "source": [
    "# Select data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of9E8ClxMNsD"
   },
   "outputs": [],
   "source": [
    "# Selecting Training Data:\n",
    "# In this code cell, we are randomly selecting a subset of data points for training.\n",
    "\n",
    "# Number of Data Points (N):\n",
    "# The variable N represents the total number of data points in the dataset.\n",
    "# It's important to note that N is hard-coded to 100 in this example.\n",
    "\n",
    "# Random Selection:\n",
    "# We use NumPy's random.choice function to randomly select 80 indices from the range [0, N) without replacement.\n",
    "trainidx = np.random.choice(range(N), 80, replace=False)\n",
    "\n",
    "# Boolean Mask:\n",
    "# We initialize a boolean vector, trainBool, with all elements set to False.\n",
    "# This vector will be used to indicate which data points are selected for training.\n",
    "\n",
    "# Mark Selected Data:\n",
    "# We set the elements at the randomly selected indices (trainidx) to True in trainBool.\n",
    "trainBool[trainidx] = True\n",
    "\n",
    "# Display Sizes:\n",
    "# We print the sizes of the selected training data (True values in trainBool) and\n",
    "# the remaining data (False values in trainBool).\n",
    "print(\"Training Data Size:\", x[trainBool].shape)\n",
    "print(\"Remaining Data Size:\", x[~trainBool].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmUXAALTRPkL"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY4ayy2VRGeZ"
   },
   "outputs": [],
   "source": [
    "# Number of Epochs:\n",
    "# The variable numepochs represents the total number of training epochs, and it is set to 500.\n",
    "\n",
    "# Training Loop:\n",
    "# We iterate over each epoch using a for loop, with epochi ranging from 0 to numepochs - 1.\n",
    "numepochs = 500\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    # Forward Pass:\n",
    "    # Inside each epoch, we perform a forward pass through the neural network model (ANNreg).\n",
    "    # We pass the input data x, which includes only the data points marked as True in trainBool.\n",
    "    # This computes the predicted values yHat.\n",
    "    yHat = ANNreg(x[trainBool])\n",
    "\n",
    "    # Compute Loss:\n",
    "    # We calculate the loss between the predicted values (yHat) and the true target values (y)\n",
    "    # for the selected training data points. The loss function used here is Mean Squared Error (MSE).\n",
    "    loss = lossfun(yHat, y[trainBool])\n",
    "\n",
    "    # Backpropagation:\n",
    "    # We then perform backpropagation to compute gradients and update the model's weights.\n",
    "    # This is done by calling optimizer.zero_grad() to reset the gradients, followed by loss.backward()\n",
    "    # to compute gradients based on the loss, and finally, optimizer.step() to update the model parameters.\n",
    "    optimizer.zero_grad()  # Reset gradients.\n",
    "    loss.backward()        # Compute gradients.\n",
    "    optimizer.step()       # Update model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmX6K49WMNuy"
   },
   "outputs": [],
   "source": [
    "# Compute Losses of the TEST Set:\n",
    "# First, we compute predictions (predYtest) for the test set. We use the trained ANNreg model\n",
    "# to make predictions for the data points that were not part of the training set (indicated by ~trainBool).\n",
    "predYtest = ANNreg(x[~trainBool])\n",
    "\n",
    "# Calculate Test Loss:\n",
    "# Next, we calculate the test loss by taking the mean squared difference between the predicted values (predYtest)\n",
    "# and the true target values (y[~trainBool]) for the test set.\n",
    "testloss = (predYtest - y[~trainBool]).pow(2).mean()\n",
    "\n",
    "# Print Out Final TRAIN and TEST Losses:\n",
    "# Finally, we print out the final training and test losses using f-strings for formatting.\n",
    "print(f'Final TRAIN loss: {loss.detach():.2f}')\n",
    "print(f'Final TEST loss: {testloss.detach():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1TCt0mpMNxC"
   },
   "outputs": [],
   "source": [
    "# Predictions for Final Training Run:\n",
    "# First, we obtain predictions (predYtrain) for the data points that were part of the training set.\n",
    "# We use the trained ANNreg model to make these predictions.\n",
    "\n",
    "predYtrain = ANNreg(x[trainBool]).detach().numpy()\n",
    "\n",
    "# Plot the Data:\n",
    "# We create a plot to visualize the data and predictions.\n",
    "# - 'x' and 'y' are plotted as black triangles ('k^') to represent all data points.\n",
    "# - Training predictions (predYtrain) are plotted as blue squares ('bs') with white markers.\n",
    "# - Test predictions (predYtest) are plotted as red circles ('ro') with white markers.\n",
    "# We also add a legend to the plot to label the different elements.\n",
    "\n",
    "plt.plot(x, y, 'k^', label='All data')\n",
    "plt.plot(x[trainBool], predYtrain, 'bs', markerfacecolor='w', label='Training pred.')\n",
    "plt.plot(x[~trainBool], predYtest.detach(), 'ro', markerfacecolor='w', label='Test pred.')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucAVWWYEbBE5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjvdKsrdZ4ka"
   },
   "source": [
    "# Additional explorations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNSoqVKdJX7oDTOd7ULis2C",
   "collapsed_sections": [],
   "name": "DUDL_overfitting_regression.ipynb",
   "provenance": [
    {
     "file_id": "1Q_oDw0aMA4QFKDnLxuqJp62P8oPMtO1R",
     "timestamp": 1616705543155
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615884593383
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
