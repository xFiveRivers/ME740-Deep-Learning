{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-YUb7pW19yy"
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# Create 'fakedata' by stacking and summing two components:\n",
    "# 1. A repeated array of values [1, 2, 3, 4] tiled 10 times vertically (10 rows).\n",
    "# 2. Another array generated by multiplying [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] by 10 and then transposing it.\n",
    "fakedata = np.tile(np.array([1, 2, 3, 4]), (10, 1)) + np.tile(10 * np.arange(1, 11), (4, 1)).T\n",
    "\n",
    "# Create 'fakelabels' by checking if each element in the range [0, 1, 2, ..., 9] is greater than 4.\n",
    "# This creates a Boolean array where 'True' corresponds to values greater than 4.\n",
    "fakelabels = np.arange(10) > 4\n",
    "\n",
    "# Print 'fakedata' and add a line break for clarity.\n",
    "print(fakedata)\n",
    "print(' ')\n",
    "\n",
    "# Print 'fakelabels'.\n",
    "print(fakelabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhkvsJ6g6uXr"
   },
   "source": [
    "# Using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bxbHGkP7JW3"
   },
   "outputs": [],
   "source": [
    "# Specify the sizes of the partitions for data splitting.\n",
    "# The order is training data, development set (devset), and test data.\n",
    "partitions = [.8, .1, .1]\n",
    "\n",
    "# Split the data into training and temporary test data using the specified training size.\n",
    "# 'testTMP' variables are temporary and will be further split into devset and test data.\n",
    "train_data, testTMP_data, train_labels, testTMP_labels = \\\n",
    "    train_test_split(fakedata, fakelabels, train_size=partitions[0])\n",
    "\n",
    "# Now, split the temporary test data into devset and test data based on the partition sizes.\n",
    "# The 'split' variable calculates the proportion of the devset relative to the remaining data.\n",
    "devset_data, test_data, devset_labels, test_labels = \\\n",
    "    train_test_split(testTMP_data, testTMP_labels, train_size=partitions[1] / np.sum(partitions[1:]))\n",
    "\n",
    "# Print out the sizes of the data partitions.\n",
    "print('Training data size: ' + str(train_data.shape))\n",
    "print('Devset data size: ' + str(devset_data.shape))\n",
    "print('Test data size: ' + str(test_data.shape))\n",
    "print(' ')\n",
    "\n",
    "# Print out the contents of each data partition.\n",
    "print('Training data: ')\n",
    "print(train_data)\n",
    "print(' ')\n",
    "\n",
    "print('Devset data: ')\n",
    "print(devset_data)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvUQFxSTV2SB"
   },
   "source": [
    "# Splitting the data manually using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUZcKNWsXg00"
   },
   "outputs": [],
   "source": [
    "# Specify sizes of the partitions\n",
    "# The order is train, devset, test\n",
    "partitions = [.8, .1, .1]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, testTMP_data, train_labels, testTMP_labels = train_test_split(fakedata, fakelabels, train_size=partitions[0])\n",
    "\n",
    "# Now split the testTMP data into devset and test sets\n",
    "# The split ratio is based on the specified partition proportions\n",
    "split = partitions[1] / np.sum(partitions[1:])\n",
    "devset_data, test_data, devset_labels, test_labels = train_test_split(testTMP_data, testTMP_labels, train_size=split)\n",
    "\n",
    "# Print out the sizes of the partitions\n",
    "print('Training data size: ' + str(train_data.shape))\n",
    "print('Devset data size: ' + str(devset_data.shape))\n",
    "print('Test data size: ' + str(test_data.shape))\n",
    "print(' ')\n",
    "\n",
    "# Print out the train, devset, and test data\n",
    "print('Training data: ')\n",
    "print(train_data)\n",
    "print(' ')\n",
    "\n",
    "print('Devset data: ')\n",
    "print(devset_data)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vre4YiQBZmjy"
   },
   "outputs": [],
   "source": [
    "# Selecting rows for the training data:\n",
    "# We use indexing to select a subset of rows from the `fakedata` and `fakelabels` arrays.\n",
    "# The indices for the training data are determined by the `randindices` array.\n",
    "# We select rows from the beginning up to `partitionBnd[0]`, which corresponds to the size of the training partition.\n",
    "train_dataN = fakedata[randindices[:partitionBnd[0]], :]\n",
    "train_labelsN = fakelabels[randindices[:partitionBnd[0]]]\n",
    "\n",
    "# Selecting rows for the devset data:\n",
    "# We continue to use indexing to select a subset of rows for the devset data.\n",
    "# Here, we start from `partitionBnd[0]` (the end of the training data) and go up to `partitionBnd[1]`, which defines the size of the devset partition.\n",
    "devset_dataN = fakedata[randindices[partitionBnd[0]:partitionBnd[1]], :]\n",
    "devset_labelsN = fakelabels[randindices[partitionBnd[0]:partitionBnd[1]]]\n",
    "\n",
    "# Selecting rows for the test data:\n",
    "# Similar to the previous two cases, we use indexing to select rows for the test data.\n",
    "# This time, we start from `partitionBnd[1]` (the end of the devset data) and select rows up to the end of the dataset.\n",
    "test_dataN = fakedata[randindices[partitionBnd[1]:], :]\n",
    "test_labelsN = fakelabels[randindices[partitionBnd[1]:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbTLW0MkXg-V"
   },
   "outputs": [],
   "source": [
    "# Printing out the sizes of the datasets:\n",
    "# We use the `shape` attribute of NumPy arrays to determine the dimensions (rows and columns) of each dataset.\n",
    "# The `train_dataN`, `devset_dataN`, and `test_dataN` arrays represent the data for training, devset, and testing.\n",
    "# We use the `str()` function to convert the shape information to a string and then concatenate it with the description.\n",
    "\n",
    "print('Training data size: ' + str(train_dataN.shape))\n",
    "print('Devset size: '        + str(devset_dataN.shape))\n",
    "print('Test data size: '     + str(test_dataN.shape))\n",
    "print(' ')\n",
    "\n",
    "# Printing out the actual data:\n",
    "# We print the contents of each dataset to inspect the values.\n",
    "# This is useful for debugging and understanding the dataset's structure and content.\n",
    "\n",
    "print('Training data: ')\n",
    "print(train_dataN)\n",
    "print(' ')\n",
    "\n",
    "print('Devset data: ')\n",
    "print(devset_dataN)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(test_dataN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaP1IQDrXhBc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyORIyTWClz7YOKdhmp8/3fQ",
   "collapsed_sections": [],
   "name": "DUDL_overfitting_trainDevsetTest.ipynb",
   "provenance": [
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
