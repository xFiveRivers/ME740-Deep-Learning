{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e145f6f8",
   "metadata": {},
   "source": [
    "# 04 - Autograd and Automatic Differentiation in PyTorch\n",
    "\n",
    "This notebook covers the concepts and functionalities related to PyTorch's autograd system for automatic differentiation. The focus is on understanding gradients and backpropagation, using `requires_grad`, `.backward()`, and `torch.autograd.grad()`, calculating higher-order gradients, and managing memory with PyTorch's dynamic computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f7bdb",
   "metadata": {},
   "source": [
    "## 1. Understanding Gradients and Backpropagation\n",
    "\n",
    "Gradients represent the partial derivatives of a function with respect to its inputs. Backpropagation is the process of computing these gradients in reverse order from the output to the input of a neural network, allowing for weight updates during training.\n",
    "In PyTorch, gradients are automatically computed for tensors with `requires_grad=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Computing Gradients with PyTorch\n",
    "import torch\n",
    "\n",
    "# Define a tensor with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2  # y = x^2\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "print('Gradient of y with respect to x:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Gradient of a Multi-variable Function\n",
    "# Define tensors\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * z + x ** 2\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "print('Gradients (dy/dx):', x.grad)\n",
    "print('Gradients (dy/dz):', z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4789d7f",
   "metadata": {},
   "source": [
    "## 2. Using `requires_grad`, `.backward()`, and `torch.autograd.grad()`\n",
    "\n",
    "These tools are essential for performing gradient computations in PyTorch:\n",
    "- **`requires_grad`**: Enables tracking of operations on a tensor.\n",
    "- **`.backward()`**: Computes the gradients of the scalar output with respect to the input.\n",
    "- **`torch.autograd.grad()`**: Provides a finer level of control over gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db091ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using torch.autograd.grad()\n",
    "# Define tensors\n",
    "a = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "# Compute gradient manually\n",
    "grad_c_a = torch.autograd.grad(c, a, retain_graph=True)\n",
    "print('Gradient of c with respect to a:', grad_c_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b330b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Using retain_graph in backward()\n",
    "# Define tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "\n",
    "# Compute first gradient\n",
    "y.backward(retain_graph=True)\n",
    "print('First Gradient:', x.grad)\n",
    "\n",
    "# Compute another gradient\n",
    "y.backward()\n",
    "print('Accumulated Gradient:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a23b67",
   "metadata": {},
   "source": [
    "## 3. Higher-Order Gradients, Jacobian, and Hessian Calculations\n",
    "\n",
    "Higher-order gradients (second derivatives, etc.) are often required in optimization algorithms. PyTorch provides tools for calculating higher-order gradients, Jacobians, and Hessians efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab55db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Calculating Higher-Order Gradients\n",
    "# Define tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "\n",
    "# Compute first derivative\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print('First Derivative:', grad1)\n",
    "\n",
    "# Compute second derivative\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "print('Second Derivative:', grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Calculating Jacobian Matrix\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# Define function for Jacobian calculation\n",
    "jacobian = torch.autograd.functional.jacobian(lambda x: x ** 2, x)\n",
    "print('Jacobian Matrix:', jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b55e31",
   "metadata": {},
   "source": [
    "## 4. Managing and Optimizing Memory Usage with PyTorch's Dynamic Computation Graph\n",
    "\n",
    "PyTorch builds a dynamic computation graph at runtime, which can help optimize memory usage. Techniques such as clearing gradients, detaching tensors, and using `torch.no_grad()` can help manage memory effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Clearing Gradients to Save Memory\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print('Gradient before clearing:', x.grad)\n",
    "x.grad.zero_()\n",
    "print('Gradient after clearing:', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Using torch.no_grad() to Prevent Memory Leaks\n",
    "# Prevents tracking of operations\n",
    "with torch.no_grad():\n",
    "    a = torch.tensor(1.0, requires_grad=True)\n",
    "    b = a * 2\n",
    "    print('Result without tracking gradients:', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: Detaching Tensors to Free Memory\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y.detach()  # Detach tensor from computation graph\n",
    "print('Detached Tensor:', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f33d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: Optimizing Memory with torch.cuda.memory_allocated()\n",
    "# Check memory usage (only relevant if CUDA is available)\n",
    "if torch.cuda.is_available():\n",
    "    print('Memory allocated on GPU:', torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52251a51",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Compute the gradient of a multi-variable function and interpret the results.\n",
    "2. Use `torch.autograd.grad()` to compute custom gradients for a specific function.\n",
    "3. Calculate the Jacobian matrix for a non-linear function.\n",
    "4. Calculate higher-order gradients for different functions and observe their values.\n",
    "5. Use `torch.no_grad()` to prevent gradient tracking during certain operations.\n",
    "6. Detach tensors from the computation graph to save memory and compare results with tracked gradients.\n",
    "7. Check memory usage before and after performing tensor operations on the GPU (if available).\n",
    "8. Implement a function that computes both the gradient and the Hessian matrix of a scalar-valued function.\n",
    "9. Explore the effect of gradient accumulation and how to manage it using `.zero_()` method.\n",
    "10. Demonstrate the use of `retain_graph` in different scenarios to manage memory usage during backpropagation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
