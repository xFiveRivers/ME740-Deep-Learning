{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c62daa",
   "metadata": {},
   "source": [
    "# 05 - Loss Functions and Optimization in PyTorch\n",
    "\n",
    "This notebook covers various loss functions and optimization techniques in PyTorch. The topics include built-in loss functions, defining custom loss functions, understanding optimization algorithms, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c14b9",
   "metadata": {},
   "source": [
    "## 1. Built-in Loss Functions in PyTorch\n",
    "\n",
    "PyTorch provides several built-in loss functions to help train neural networks. Common loss functions include:\n",
    "- **Mean Squared Error Loss (MSELoss)**: Used for regression tasks.\n",
    "- **Cross Entropy Loss (CrossEntropyLoss)**: Used for classification tasks.\n",
    "- **Negative Log Likelihood Loss (NLLLoss)**: Often used in conjunction with softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Mean Squared Error Loss (MSELoss)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define target and prediction\n",
    "target = torch.tensor([1.0, 2.0, 3.0])\n",
    "prediction = torch.tensor([1.1, 2.2, 3.3])\n",
    "\n",
    "# MSE Loss\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(prediction, target)\n",
    "print('MSE Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Cross Entropy Loss (CrossEntropyLoss)\n",
    "# Define target and prediction\n",
    "target = torch.tensor([1])  # Target class index\n",
    "prediction = torch.tensor([[1.5, 0.5, 2.1]])  # Logits (not probabilities)\n",
    "\n",
    "# Cross Entropy Loss\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(prediction, target)\n",
    "print('Cross Entropy Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63efabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Negative Log Likelihood Loss (NLLLoss)\n",
    "# Define target and prediction\n",
    "log_probs = torch.tensor([[-0.5, -0.6, -1.2]])  # Log probabilities\n",
    "target = torch.tensor([2])  # Target class index\n",
    "\n",
    "# NLL Loss\n",
    "nll_loss = nn.NLLLoss()\n",
    "loss = nll_loss(log_probs, target)\n",
    "print('Negative Log Likelihood Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2873752",
   "metadata": {},
   "source": [
    "## 2. Defining Custom Loss Functions\n",
    "\n",
    "Custom loss functions can be defined in PyTorch using the `torch.nn.Module` class. This is useful when standard loss functions do not fit the specific requirements of the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Defining a Custom Loss Function\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return torch.mean((output - target) ** 2) + torch.sum(torch.abs(output - target))\n",
    "\n",
    "# Use the custom loss\n",
    "custom_loss = CustomLoss()\n",
    "output = torch.tensor([0.0, 0.5, 0.8])\n",
    "target = torch.tensor([0.0, 1.0, 1.0])\n",
    "loss = custom_loss(output, target)\n",
    "print('Custom Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970eee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Another Custom Loss Function\n",
    "class CustomL1L2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomL1L2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        l1 = torch.sum(torch.abs(output - target))\n",
    "        l2 = torch.sum((output - target) ** 2)\n",
    "        return l1 + l2\n",
    "\n",
    "# Use the custom loss\n",
    "custom_loss = CustomL1L2Loss()\n",
    "loss = custom_loss(output, target)\n",
    "print('Custom L1 + L2 Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717cb5d",
   "metadata": {},
   "source": [
    "## 3. Understanding Optimization Algorithms and Using Optimizers\n",
    "\n",
    "Optimization algorithms are crucial for training neural networks as they adjust model weights to minimize the loss function. Common optimizers in PyTorch include:\n",
    "- **Stochastic Gradient Descent (SGD)**\n",
    "- **Adam**\n",
    "- **RMSprop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Using SGD Optimizer\n",
    "model = nn.Linear(2, 1)  # Simple linear model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy training loop\n",
    "for epoch in range(3):\n",
    "    # Forward pass\n",
    "    output = model(torch.tensor([[1.0, 2.0]]))\n",
    "    loss = mse_loss(output, torch.tensor([1.0]))\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Using Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy training loop\n",
    "for epoch in range(3):\n",
    "    output = model(torch.tensor([[1.0, 2.0]]))\n",
    "    loss = mse_loss(output, torch.tensor([1.0]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f3f66",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Scheduling and Dynamic Adjustments\n",
    "\n",
    "Learning rate scheduling involves dynamically adjusting the learning rate during training to improve convergence. PyTorch provides several schedulers like `StepLR` and `ReduceLROnPlateau`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb31d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Using StepLR Scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "# Dummy training loop\n",
    "for epoch in range(3):\n",
    "    output = model(torch.tensor([[1.0, 2.0]]))\n",
    "    loss = mse_loss(output, torch.tensor([1.0]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Learning Rate: {scheduler.get_last_lr()}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
