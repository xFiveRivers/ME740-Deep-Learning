{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5ed249",
   "metadata": {},
   "source": [
    "# Additional PyTorch Operations and Functions for ANNs\n",
    "\n",
    "This notebook covers more specialized PyTorch operations and built-in functions commonly used in Artificial Neural Networks (ANNs). The focus is on advanced functionalities, such as custom loss functions, advanced optimizers, gradient clipping, and various utility functions that enhance model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e56be",
   "metadata": {},
   "source": [
    "## 1. Custom Loss Functions\n",
    "\n",
    "While PyTorch provides several built-in loss functions, there are cases where custom loss functions are required. This section demonstrates how to define custom loss functions using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom Loss Function\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom loss function class\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.mean((output - target) ** 2)  # Mean squared error\n",
    "        return loss\n",
    "\n",
    "# Use the custom loss function\n",
    "custom_loss = CustomMSELoss()\n",
    "output = torch.tensor([0.0, 0.5, 0.8], requires_grad=True)\n",
    "target = torch.tensor([0.0, 1.0, 1.0])\n",
    "loss = custom_loss(output, target)\n",
    "print('Custom MSE Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5875181",
   "metadata": {},
   "source": [
    "## 2. Advanced Optimizers\n",
    "\n",
    "PyTorch includes a variety of optimizers beyond the commonly used SGD and Adam. These advanced optimizers can provide better performance in specific scenarios. Examples include `Adagrad`, `RMSprop`, and `AdamW`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Advanced Optimizers\n",
    "model = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))\n",
    "optimizer_adagrad = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "optimizer_rmsprop = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "optimizer_adamw = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "print('Adagrad Optimizer:', optimizer_adagrad)\n",
    "print('RMSprop Optimizer:', optimizer_rmsprop)\n",
    "print('AdamW Optimizer:', optimizer_adamw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c4b27",
   "metadata": {},
   "source": [
    "## 3. Gradient Clipping\n",
    "\n",
    "Gradient clipping is used to prevent the exploding gradient problem by limiting the magnitude of the gradients during backpropagation. PyTorch provides `torch.nn.utils.clip_grad_norm_` for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e88b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Applying Gradient Clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    output = model(torch.randn(10))  # Forward pass\n",
    "    loss = output.sum()  # Example loss\n",
    "    loss.backward()  # Backward pass\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a048c5",
   "metadata": {},
   "source": [
    "## 4. Utility Functions for Model Evaluation\n",
    "\n",
    "PyTorch provides several utility functions to assist in model evaluation, such as accuracy calculation, confusion matrix generation, and other metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating Accuracy\n",
    "def calculate_accuracy(output, target):\n",
    "    _, predictions = torch.max(output, 1)\n",
    "    correct = (predictions == target).sum().item()\n",
    "    accuracy = correct / target.size(0)\n",
    "    return accuracy\n",
    "\n",
    "output = torch.tensor([[0.1, 0.9], [0.8, 0.2], [0.4, 0.6]])\n",
    "target = torch.tensor([1, 0, 1])\n",
    "accuracy = calculate_accuracy(output, target)\n",
    "print('Model Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151f19f",
   "metadata": {},
   "source": [
    "## 5. Saving and Loading Checkpoints\n",
    "\n",
    "Checkpoints are snapshots of a model's state at a particular point during training. Saving and loading checkpoints is essential for long training processes to resume training or evaluate a model's performance at a specific state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fe1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving and Loading Checkpoints\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'epoch': epoch}\n",
    "torch.save(checkpoint, 'checkpoint.pth')  # Save checkpoint\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "print('Checkpoint Loaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9bcf9a",
   "metadata": {},
   "source": [
    "## 6. Early Stopping\n",
    "\n",
    "Early stopping is a technique used to terminate training when the model stops improving on a validation set. This prevents overfitting and reduces training time. Although PyTorch does not have a built-in early stopping function, it is straightforward to implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ca1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Early Stopping Implementation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Example usage of EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "for epoch in range(10):\n",
    "    val_loss = torch.randn(1).item()  # Simulate a random validation loss\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss}')\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6c096",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement a custom loss function for a specific problem.\n",
    "2. Use an advanced optimizer and observe its effects on the training process.\n",
    "3. Apply gradient clipping and monitor the changes in the loss function.\n",
    "4. Save and load model checkpoints to resume training from a specific point.\n",
    "5. Implement early stopping and test it on a training loop."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
