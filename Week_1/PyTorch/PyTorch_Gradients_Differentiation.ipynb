{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0085567e",
   "metadata": {},
   "source": [
    "# PyTorch Operations for Gradients and Differentiation\n",
    "\n",
    "This notebook explores PyTorch's functionalities related to gradients and automatic differentiation. PyTorch's `autograd` package provides automatic differentiation for all operations on Tensors. This is particularly useful for deep learning, where gradients are used to optimize the weights of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add210d4",
   "metadata": {},
   "source": [
    "## 1. Understanding Tensors with `requires_grad`\n",
    "\n",
    "In PyTorch, a tensor can be created with the `requires_grad` flag set to `True`. This flag indicates that all operations on the tensor should be tracked for gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a Tensor with requires_grad\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print('Tensor:', x)\n",
    "print('Requires Grad:', x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059d6aa",
   "metadata": {},
   "source": [
    "## 2. Gradient Calculation using `backward()`\n",
    "\n",
    "Gradients in PyTorch are calculated using the `.backward()` method. This method computes the derivatives of a scalar value (usually the loss) with respect to the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing Gradients\n",
    "# Define a simple function\n",
    "y = x[0] ** 2 + 3 * x[1]\n",
    "print('Function y:', y)\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "print('Gradient of x:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18f473",
   "metadata": {},
   "source": [
    "## 3. Zeroing Out Gradients\n",
    "\n",
    "Gradients in PyTorch accumulate by default, meaning they are added to existing gradients. To avoid this during each training step, gradients should be manually zeroed out using the `.zero_()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Zeroing Out Gradients\n",
    "x.grad.zero_()  # Clear the existing gradients\n",
    "print('Gradients after zeroing:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae8d7e",
   "metadata": {},
   "source": [
    "## 4. Stopping Gradient Computation\n",
    "\n",
    "Sometimes, gradients need to be frozen or excluded from computation. This can be done using the `torch.no_grad()` context or `.detach()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Stopping Gradient Computation\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print('Tensor z with no gradient tracking:', z)\n",
    "\n",
    "# Using detach()\n",
    "z_detach = x.detach()\n",
    "print('Detached Tensor:', z_detach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b07be",
   "metadata": {},
   "source": [
    "## 5. Higher-Order Gradients\n",
    "\n",
    "PyTorch supports higher-order gradients, which means calculating the gradient of a gradient. This is useful in certain advanced machine learning models and optimization algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e094a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Higher-Order Gradients\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x[0] ** 3 + 2 * x[1] ** 2\n",
    "\n",
    "# First derivative\n",
    "y.backward(retain_graph=True)\n",
    "print('First-order Gradients:', x.grad)\n",
    "\n",
    "# Second derivative\n",
    "gradients = torch.autograd.grad(y, x, create_graph=True)\n",
    "second_derivatives = torch.autograd.grad(gradients[0][0], x, retain_graph=True)\n",
    "print('Second-order Gradients:', second_derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7ece2",
   "metadata": {},
   "source": [
    "## 6. Using `torch.autograd.grad()` for Custom Derivative Calculations\n",
    "\n",
    "`torch.autograd.grad()` provides a finer level of control over gradient calculation, allowing calculation of gradients of specific tensors with respect to others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom Gradient Calculation\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "c = a * b\n",
    "grad_c_a = torch.autograd.grad(c, a, grad_outputs=torch.tensor([1.0, 1.0, 1.0]))\n",
    "print('Custom Gradients of c with respect to a:', grad_c_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf1121",
   "metadata": {},
   "source": [
    "## 7. Jacobian and Hessian Computations\n",
    "\n",
    "Jacobian and Hessian matrices are often used in optimization and advanced machine learning techniques. PyTorch allows for efficient computation of these matrices using the `autograd` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Jacobian Computation\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x[0] ** 2 + 2 * x[1] ** 2\n",
    "\n",
    "# Compute the Jacobian\n",
    "jacobian = torch.autograd.functional.jacobian(lambda x: x[0] ** 2 + 2 * x[1] ** 2, x)\n",
    "print('Jacobian:', jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8640c",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create tensors with `requires_grad` set to `True` and compute their gradients.\n",
    "2. Use the `.zero_()` method to clear gradients and verify its effect.\n",
    "3. Practice stopping gradient computation using `torch.no_grad()` and `.detach()`.\n",
    "4. Calculate higher-order gradients and understand their applications.\n",
    "5. Use `torch.autograd.grad()` to compute custom gradients for different tensors.\n",
    "6. Compute Jacobian and Hessian matrices for different functions."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
