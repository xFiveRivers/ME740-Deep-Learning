{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f462c2c1",
   "metadata": {},
   "source": [
    "# Additional PyTorch Functions and Operations\n",
    "\n",
    "This notebook focuses on additional built-in functions and operations in PyTorch that are useful for various machine learning and deep learning tasks. These functions include tensor reshaping, padding, masking, element-wise functions, and more. The goal is to provide comprehensive examples and explanations to help students understand and effectively use these functions in their PyTorch workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3615f7d",
   "metadata": {},
   "source": [
    "## 1. Tensor Reshaping and Slicing\n",
    "\n",
    "Reshaping and slicing tensors are essential operations in PyTorch, allowing for the transformation and manipulation of tensor shapes to fit specific requirements in neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reshaping Tensors\n",
    "x = torch.arange(12)\n",
    "reshaped_x = x.reshape(3, 4)\n",
    "print('Original Tensor:', x)\n",
    "print('Reshaped Tensor (3x4):', reshaped_x)\n",
    "\n",
    "# Example: Slicing Tensors\n",
    "print('Sliced Tensor:', reshaped_x[:, 1:3])  # Slicing columns 1 to 3\n",
    "print('Selected Elements:', reshaped_x[1, :])  # Selecting the second row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb38fa",
   "metadata": {},
   "source": [
    "## 2. Padding and Masking\n",
    "\n",
    "Padding tensors is often required when dealing with sequences of varying lengths. Masking is useful in tasks where certain values need to be ignored or skipped, such as in NLP or attention models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Padding Tensors\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "padded_x = F.pad(x, pad=(1, 1, 0, 0), mode='constant', value=0)\n",
    "print('Original Tensor:', x)\n",
    "print('Padded Tensor:', padded_x)\n",
    "\n",
    "# Example: Masking Tensors\n",
    "mask = (x > 3)\n",
    "masked_tensor = x.masked_select(mask)\n",
    "print('Masked Elements Greater Than 3:', masked_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cde3e",
   "metadata": {},
   "source": [
    "## 3. Element-wise Functions\n",
    "\n",
    "PyTorch provides several element-wise functions that operate on each element of the input tensor. These include mathematical functions (such as `sqrt`, `log`, `exp`) and comparison operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Element-wise Mathematical Functions\n",
    "x = torch.tensor([1.0, 4.0, 9.0])\n",
    "sqrt_x = torch.sqrt(x)\n",
    "log_x = torch.log(x)\n",
    "exp_x = torch.exp(x)\n",
    "print('Square Root:', sqrt_x)\n",
    "print('Logarithm:', log_x)\n",
    "print('Exponential:', exp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78845481",
   "metadata": {},
   "source": [
    "## 4. Comparison Operations\n",
    "\n",
    "Comparison operations are useful for evaluating and comparing tensors, often in conditional statements or filtering operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d542ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparison Operations\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([2.0, 2.0, 2.0])\n",
    "greater_than = torch.gt(x, y)  # Greater than comparison\n",
    "equal_to = torch.eq(x, y)  # Equality comparison\n",
    "print('Greater Than Comparison:', greater_than)\n",
    "print('Equality Comparison:', equal_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc07872",
   "metadata": {},
   "source": [
    "## 5. Sorting and Indexing\n",
    "\n",
    "Sorting and indexing functions in PyTorch allow for the arrangement and selection of tensor elements based on specific criteria. This is particularly useful for tasks such as top-k selection, ranking, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sorting and Indexing\n",
    "x = torch.tensor([3, 1, 4, 2])\n",
    "sorted_x, indices = torch.sort(x)\n",
    "print('Sorted Tensor:', sorted_x)\n",
    "print('Original Indices After Sort:', indices)\n",
    "\n",
    "# Top-k Selection\n",
    "top_k_values, top_k_indices = torch.topk(x, 2)\n",
    "print('Top-k Values:', top_k_values)\n",
    "print('Top-k Indices:', top_k_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a280751",
   "metadata": {},
   "source": [
    "## 6. Broadcasting Rules\n",
    "\n",
    "Broadcasting allows PyTorch to perform operations on tensors of different shapes by 'stretching' them to compatible shapes. Understanding broadcasting rules is essential for efficient tensor operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf310ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Broadcasting\n",
    "x = torch.tensor([[1], [2], [3]])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "broadcasted_sum = x + y  # Broadcasting the addition\n",
    "print('Broadcasted Sum:', broadcasted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d29bb",
   "metadata": {},
   "source": [
    "## 7. Other Useful Functions\n",
    "\n",
    "There are several other built-in functions in PyTorch that are extremely useful for various tasks:\n",
    "- **Clamp**: Restricting values in a tensor to a specified range.\n",
    "- **Unique**: Finding unique elements in a tensor.\n",
    "- **Cat and Stack**: Concatenating or stacking tensors along a specified dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Clamping and Finding Unique Elements\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "clamped_x = torch.clamp(x, min=2.0, max=4.0)\n",
    "print('Clamped Tensor:', clamped_x)\n",
    "\n",
    "y = torch.tensor([1, 2, 2, 3, 4, 4, 4])\n",
    "unique_y = torch.unique(y)\n",
    "print('Unique Elements in Tensor:', unique_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Concatenation and Stacking\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6]])\n",
    "\n",
    "# Concatenate along rows\n",
    "concatenated = torch.cat((a, b), dim=0)\n",
    "print('Concatenated Tensor:', concatenated)\n",
    "\n",
    "# Stack along a new dimension\n",
    "stacked = torch.stack((a, a), dim=0)\n",
    "print('Stacked Tensor:', stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5b35d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Reshape and slice tensors to extract specific elements.\n",
    "2. Apply padding and masking to manipulate input data.\n",
    "3. Use element-wise functions to perform mathematical operations.\n",
    "4. Perform comparison operations to filter data based on conditions.\n",
    "5. Sort a tensor and select the top-k elements.\n",
    "6. Experiment with broadcasting rules to perform operations on tensors of different shapes.\n",
    "7. Use `clamp`, `unique`, `cat`, and `stack` functions to manipulate tensors."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
