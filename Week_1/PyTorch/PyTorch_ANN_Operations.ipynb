{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce04516",
   "metadata": {},
   "source": [
    "# PyTorch Operations for Artificial Neural Networks (ANNs)\n",
    "\n",
    "This notebook covers essential PyTorch operations and functions specifically useful for building Artificial Neural Networks (ANNs). The focus is on operations that are frequently used in ANN development, including layer definitions, weight initialization, dropout, batch normalization, and various utility functions. The examples and explanations aim to help students understand and effectively apply these operations in their neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535cd45",
   "metadata": {},
   "source": [
    "## 1. Defining Layers for ANN\n",
    "\n",
    "In PyTorch, neural network layers are defined using the `torch.nn` module. This module provides a variety of pre-built layers such as fully connected layers (`Linear`), convolutional layers, pooling layers, etc. Here we focus on the fully connected layers used in ANNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d13489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Defining a Fully Connected Layer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a single fully connected layer\n",
    "fc_layer = nn.Linear(in_features=10, out_features=5)  # Input size 10, output size 5\n",
    "print('Fully Connected Layer:', fc_layer)\n",
    "print('Layer Weight Shape:', fc_layer.weight.shape)\n",
    "print('Layer Bias Shape:', fc_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9bc55",
   "metadata": {},
   "source": [
    "## 2. Weight Initialization for ANN\n",
    "\n",
    "Weight initialization is critical for training deep learning models. Proper initialization can help avoid problems like vanishing or exploding gradients. PyTorch provides several initialization methods, including Xavier (Glorot) and Kaiming (He) initializations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed08088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weight Initialization\n",
    "nn.init.xavier_uniform_(fc_layer.weight)\n",
    "print('Xavier Initialized Weights:', fc_layer.weight)\n",
    "\n",
    "nn.init.kaiming_uniform_(fc_layer.weight, nonlinearity='relu')\n",
    "print('Kaiming Initialized Weights:', fc_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18a903",
   "metadata": {},
   "source": [
    "## 3. Dropout for Regularization\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero during training. PyTorch provides the `nn.Dropout` layer to implement dropout easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Applying Dropout\n",
    "dropout_layer = nn.Dropout(p=0.5)  # Dropout probability of 50%\n",
    "input_tensor = torch.randn(5, 10)  # A batch of 5 samples, each of dimension 10\n",
    "output_tensor = dropout_layer(input_tensor)\n",
    "print('Input Tensor:', input_tensor)\n",
    "print('Output Tensor with Dropout Applied:', output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ca83b",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "\n",
    "Batch normalization is a technique to stabilize and accelerate the training process by normalizing the input of each layer. This reduces the internal covariate shift and allows for higher learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ebe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Applying Batch Normalization\n",
    "batch_norm_layer = nn.BatchNorm1d(num_features=10)  # For a fully connected layer with 10 features\n",
    "normalized_output = batch_norm_layer(input_tensor)\n",
    "print('Batch Normalized Output:', normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b37bc",
   "metadata": {},
   "source": [
    "## 5. Activation Functions in ANN\n",
    "\n",
    "Activation functions introduce non-linearity into the model, allowing it to learn complex patterns. Commonly used activation functions in ANNs include ReLU, Sigmoid, and Tanh, all available in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Activation Functions\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "relu_output = relu(input_tensor)\n",
    "sigmoid_output = sigmoid(input_tensor)\n",
    "tanh_output = tanh(input_tensor)\n",
    "\n",
    "print('ReLU Output:', relu_output)\n",
    "print('Sigmoid Output:', sigmoid_output)\n",
    "print('Tanh Output:', tanh_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578972b",
   "metadata": {},
   "source": [
    "## 6. Utility Functions for ANNs\n",
    "\n",
    "Several utility functions are essential for building and training ANNs. These include functions for saving and loading models, shuffling datasets, and adjusting learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving and Loading Models\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2)\n",
    ")\n",
    "torch.save(model.state_dict(), 'ann_model.pth')  # Save model state\n",
    "\n",
    "# Load the saved model state\n",
    "model_loaded = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2)\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load('ann_model.pth'))\n",
    "print('Model Loaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e179a4",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Scheduling\n",
    "\n",
    "Learning rate scheduling involves dynamically adjusting the learning rate during training to improve convergence. PyTorch provides several schedulers like `StepLR`, `ExponentialLR`, and `ReduceLROnPlateau`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817eaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Learning Rate Scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Simulate training loop\n",
    "for epoch in range(20):\n",
    "    # ... training logic ...\n",
    "    print(f'Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f96bd8",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Define different types of layers (e.g., fully connected, dropout, batch normalization) and combine them into a small ANN.\n",
    "2. Initialize weights using different methods and observe the effects on training.\n",
    "3. Apply different activation functions and see their impact on model outputs.\n",
    "4. Use a learning rate scheduler to adjust learning rates dynamically during training.\n",
    "5. Experiment with saving and loading models to understand the workflow in PyTorch."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
