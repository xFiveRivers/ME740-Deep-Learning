{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d45bfd1",
   "metadata": {},
   "source": [
    "# PyTorch Operations and Functions for Deep Learning\n",
    "\n",
    "This notebook explores additional PyTorch operations and built-in functions that are particularly useful for deep learning applications. These include various initialization methods, normalization techniques, activation functions, loss functions, and more. The examples aim to give students hands-on experience with the operations most relevant to building deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2fde7",
   "metadata": {},
   "source": [
    "## 1. Initialization Methods\n",
    "\n",
    "Weight initialization is crucial for deep learning models as it can impact the convergence speed and final model performance. PyTorch provides several methods for initializing weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weight Initialization\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize a linear layer\n",
    "linear_layer = nn.Linear(5, 3)\n",
    "\n",
    "# Apply Xavier (Glorot) initialization\n",
    "nn.init.xavier_uniform_(linear_layer.weight)\n",
    "print('Xavier Initialized Weights:', linear_layer.weight)\n",
    "\n",
    "# Apply Kaiming (He) initialization\n",
    "nn.init.kaiming_uniform_(linear_layer.weight, nonlinearity='relu')\n",
    "print('Kaiming Initialized Weights:', linear_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7143c5d",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the model, enabling it to learn complex patterns. PyTorch provides several built-in activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87497f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Activation Functions\n",
    "activation_input = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ReLU Activation\n",
    "relu_output = torch.relu(activation_input)\n",
    "print('ReLU Output:', relu_output)\n",
    "\n",
    "# Sigmoid Activation\n",
    "sigmoid_output = torch.sigmoid(activation_input)\n",
    "print('Sigmoid Output:', sigmoid_output)\n",
    "\n",
    "# Tanh Activation\n",
    "tanh_output = torch.tanh(activation_input)\n",
    "print('Tanh Output:', tanh_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804efdaf",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "\n",
    "Loss functions measure the discrepancy between the predicted output and the actual target, guiding the model's optimization process. PyTorch offers various loss functions that are suitable for different tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loss Functions\n",
    "# Mean Squared Error Loss\n",
    "mse_loss = nn.MSELoss()\n",
    "output = torch.tensor([0.0, 0.5, 0.8])\n",
    "target = torch.tensor([0.0, 1.0, 1.0])\n",
    "loss = mse_loss(output, target)\n",
    "print('MSE Loss:', loss.item())\n",
    "\n",
    "# Cross Entropy Loss\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "predictions = torch.tensor([[0.1, 0.9], [0.8, 0.2], [0.4, 0.6]])\n",
    "labels = torch.tensor([1, 0, 1])\n",
    "loss = cross_entropy_loss(predictions, labels)\n",
    "print('Cross Entropy Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ace85",
   "metadata": {},
   "source": [
    "## 4. Normalization Techniques\n",
    "\n",
    "Normalization techniques such as Batch Normalization and Layer Normalization help stabilize the learning process and improve model performance by reducing internal covariate shift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5862b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Normalization Techniques\n",
    "# Batch Normalization\n",
    "batch_norm = nn.BatchNorm1d(3)\n",
    "input_data = torch.randn(10, 3)  # Batch size of 10, 3 features\n",
    "normalized_data = batch_norm(input_data)\n",
    "print('Batch Normalized Data:', normalized_data)\n",
    "\n",
    "# Layer Normalization\n",
    "layer_norm = nn.LayerNorm(3)\n",
    "normalized_data = layer_norm(input_data)\n",
    "print('Layer Normalized Data:', normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef5d34",
   "metadata": {},
   "source": [
    "## 5. Utility Functions for Deep Learning\n",
    "\n",
    "PyTorch provides several utility functions that are helpful for deep learning tasks, such as converting to one-hot encoding, shuffling datasets, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Utility Functions\n",
    "# One-hot Encoding\n",
    "labels = torch.tensor([0, 1, 2])\n",
    "one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=3)\n",
    "print('One-hot Encoded Labels:', one_hot_labels)\n",
    "\n",
    "# Shuffling a dataset\n",
    "dataset = torch.arange(10)\n",
    "shuffled_indices = torch.randperm(len(dataset))\n",
    "shuffled_dataset = dataset[shuffled_indices]\n",
    "print('Shuffled Dataset:', shuffled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2a15c",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation\n",
    "\n",
    "Data augmentation is a technique to increase the diversity of data available for training models without collecting new data. PyTorch's torchvision library provides several data augmentation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f096b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data Augmentation\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Example image (assuming PIL Image is available)\n",
    "image = Image.new('RGB', (100, 100), color = 'red')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "augmented_image = transform(image)\n",
    "augmented_image.show()  # Display the augmented image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd71da",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Initialize a neural network layer with Xavier and Kaiming initializations and compare their effects on training.\n",
    "2. Experiment with different activation functions and understand their impact on the model's output.\n",
    "3. Implement and test different loss functions on a small dataset.\n",
    "4. Apply batch normalization and layer normalization to a sample dataset and observe the changes.\n",
    "5. Use utility functions to shuffle and preprocess data before feeding it to a model.\n",
    "6. Apply data augmentation techniques using torchvision transforms to a dataset."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
